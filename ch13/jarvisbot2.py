# í•„ìš”í•œ íŒ¨í‚¤ì§€ë“¤ import
import streamlit as st  # Streamlit íŒ¨í‚¤ì§€ ë¶ˆëŸ¬ì˜¤ê¸°
from audiorecorder import audiorecorder  # ìŒì„± ë…¹ìŒ ê¸°ëŠ¥ì„ ìœ„í•œ íŒ¨í‚¤ì§€
from openai import OpenAI  # OpenAI API ì‚¬ìš©ì„ ìœ„í•œ íŒ¨í‚¤ì§€
import os  # íŒŒì¼ ì²˜ë¦¬ë¥¼ ìœ„í•œ os íŒ¨í‚¤ì§€
from datetime import datetime  # ì‹œê°„ ì •ë³´ë¥¼ ë‹¤ë£¨ê¸° ìœ„í•œ datetime íŒ¨í‚¤ì§€
import numpy as np  # ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ë¹„êµí•˜ê¸° ìœ„í•œ numpy íŒ¨í‚¤ì§€
import base64  # ìŒì› íŒŒì¼ì„ base64ë¡œ ì¸ì½”ë”©í•˜ê¸° ìœ„í•œ íŒ¨í‚¤ì§€
import requests  # HTTP ìš”ì²­ì„ ë³´ë‚´ê¸° ìœ„í•œ requests íŒ¨í‚¤ì§€
import json  # JSON ë°ì´í„°ë¥¼ ë‹¤ë£¨ê¸° ìœ„í•œ json íŒ¨í‚¤ì§€
import re  # ì •ê·œí‘œí˜„ì‹ì„ ìœ„í•œ re íŒ¨í‚¤ì§€

# ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜ ì •ì˜
def convert_speech_to_text(audio_data, openai_client):
    # ìŒì„± íŒŒì¼ì„ 'speech_input.mp3'ë¡œ ì €ì¥
    file_name = 'speech_input.mp3'
    with open(file_name, "wb") as audio_file:
        audio_file.write(audio_data.export().read())

    # ì €ì¥ëœ ìŒì„± íŒŒì¼ì„ ì—´ê¸°
    with open(file_name, "rb") as audio_file:
        try:
            # OpenAIì˜ Whisper APIë¥¼ ì‚¬ìš©í•˜ì—¬ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            transcription = openai_client.audio.transcriptions.create(
                model="whisper-1",
                file=audio_file,
                response_format="text"
            )
            # ë³€í™˜ì´ ì™„ë£Œë˜ë©´ íŒŒì¼ ì‚­ì œ
            os.remove(file_name)
        except:
            transcription = 'API Keyë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”'
    return transcription

# í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜ ì •ì˜
def convert_text_to_speech(response_text, openai_client):
    # OpenAI TTS ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜
    tts_response = openai_client.audio.speech.create(
        model="tts-1",
        voice="nova",
        input=response_text
    )
    output_file = "speech_output.mp3"
    tts_response.stream_to_file(output_file)

    # ë³€í™˜ëœ ìŒì„± íŒŒì¼ì„ ìë™ ì¬ìƒí•˜ê¸° ìœ„í•´ base64ë¡œ ì¸ì½”ë”©
    with open(output_file, "rb") as audio_file:
        audio_data = audio_file.read()
        base64_audio = base64.b64encode(audio_data).decode()
        audio_html = f"""
            <audio autoplay="True">
            <source src="data:audio/mp3;base64,{base64_audio}" type="audio/mp3">
            </audio>
        """
        st.markdown(audio_html, unsafe_allow_html=True)
    # ì¬ìƒ í›„ íŒŒì¼ ì‚­ì œ
    os.remove(output_file)

# GPT-3.5 ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì–»ëŠ” í•¨ìˆ˜ ì •ì˜
def get_gpt_response(prompt_list, openai_client):
    response = openai_client.chat.completions.create(model='gpt-3.5-turbo', messages=prompt_list)
    return response.choices[0].message.content

# ë‚ ì”¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ ì •ì˜
def get_weather_info(city_name, api_key):
    base_url = "http://api.openweathermap.org/data/2.5/weather?"
    complete_url = base_url + "q=" + city_name + "&appid=" + api_key + "&units=metric&lang=kr"
    response = requests.get(complete_url)
    weather_data = response.json()
    
    if weather_data["cod"] != "404":
        main = weather_data["main"]
        weather = weather_data["weather"][0]
        weather_description = weather["description"]
        temperature = main["temp"]
        humidity = main["humidity"]
        weather_response = f"í˜„ì¬ {city_name}ì˜ ë‚ ì”¨ëŠ” {weather_description}ì´ë©°, ì˜¨ë„ëŠ” {temperature}ë„, ìŠµë„ëŠ” {humidity}%ì…ë‹ˆë‹¤."
    else:
        weather_response = "ë„ì‹œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë„ì‹œ ì´ë¦„ì„ í™•ì¸í•´ì£¼ì„¸ìš”."
    
    return weather_response

# ì‚¬ìš©ìì˜ ì§ˆë¬¸ì—ì„œ ë„ì‹œ ì´ë¦„ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜ ì •ì˜
def extract_city_name(user_question):
    # ì •ê·œ í‘œí˜„ì‹ì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ìê°€ ë§í•œ ë„ì‹œ ì´ë¦„ ì¶”ì¶œ (ì˜ˆ: "ì„œìš¸")
    city_name_match = re.search(r'ì§€ê¸ˆ\s*(.*?)\s*ì˜\s*ë‚ ì”¨', user_question)
    if city_name_match:
        return city_name_match.group(1)
    return None

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ì¸ê³µì§€ëŠ¥ ë¹„ì„œ ìë¹„ìŠ¤(ë‚ ì”¨ ì¶”ê°€)",
    layout="wide"
)

# session state ì´ˆê¸°í™”
if "conversation" not in st.session_state:
    st.session_state["conversation"] = []

if "previous_audio" not in st.session_state:
    st.session_state["previous_audio"] = []

if "prompts" not in st.session_state:
    st.session_state["prompts"] = [{"role": "system", "content": 'You are a thoughtful assistant. Respond to all input in 25 words and answer in korean'}]

# í˜ì´ì§€ ì œëª© ì„¤ì •
st.header('ì¸ê³µì§€ëŠ¥ ë¹„ì„œ ìë¹„ìŠ¤ì…ë‹ˆë‹¤')
st.subheader('ì œê°€ ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? ğŸ¤–')
st.markdown('---')

# OpenAI API í‚¤ ì§€ì •
openai_client = OpenAI(
    api_key = "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"
)

# OpenWeatherMap API í‚¤ ì§€ì •
weather_api_key = "a525268a4f284f71278ab51ab757924b"  # ì—¬ê¸°ì— ì‹¤ì œ OpenWeatherMap API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”

start_flag = False

# Streamlit ë ˆì´ì•„ì›ƒ ì„¤ì •
left_column, right_column = st.columns(2)
with left_column:
    # ìŒì„± ë…¹ìŒ ê¸°ëŠ¥
    recorded_audio = audiorecorder("ì§ˆë¬¸í•˜ê¸°", "ë“£ê³  ìˆëŠ” ì¤‘ì…ë‹ˆë‹¤...")
    if len(recorded_audio) > 0 and not np.array_equal(recorded_audio, st.session_state["previous_audio"]):
        # ë…¹ìŒëœ ìŒì„±ì„ ì¬ìƒ
        st.audio(recorded_audio.export().read())

        # ë…¹ìŒëœ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        user_question = convert_speech_to_text(recorded_audio, openai_client)

        # ëŒ€í™” ë‚´ìš©ì„ ê¸°ë¡í•˜ê¸° ìœ„í•´ í˜„ì¬ ì‹œê°„ì„ ê°€ì ¸ì˜¤ê¸°
        current_time = datetime.now().strftime("%H:%M")
        st.session_state["conversation"] = st.session_state["conversation"] + [("user", current_time, user_question)]
        
        # GPT ëª¨ë¸ì— ì…ë ¥í•  í”„ë¡¬í”„íŠ¸ ì—…ë°ì´íŠ¸
        st.session_state["prompts"] = st.session_state["prompts"] + [{"role": "user", "content": user_question}]
        
        # í˜„ì¬ ë…¹ìŒëœ ìŒì„±ì„ ì €ì¥í•˜ì—¬ ë‹¤ìŒ ë…¹ìŒê³¼ ë¹„êµ
        st.session_state["previous_audio"] = recorded_audio
        start_flag = True

with right_column:
    # ëŒ€í™” ê¸°ë¡ì„ ìœ„í•œ ê³µê°„
    st.image('ai.png', width=100)
    st.subheader('ëŒ€í™”ê¸°ë¡ âŒ¨')
    if start_flag:
        # GPT ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ ì–»ê¸°
        ai_response = get_gpt_response(st.session_state["prompts"], openai_client)
        
        # ì‚¬ìš©ìê°€ ë‚ ì”¨ ì •ë³´ë¥¼ ìš”ì²­í–ˆëŠ”ì§€ í™•ì¸
        city_name = extract_city_name(user_question)
        if city_name:
            # OpenWeatherMap APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë‚ ì”¨ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            ai_response = get_weather_info(city_name, weather_api_key)

        # í”„ë¡¬í”„íŠ¸ ì—…ë°ì´íŠ¸
        st.session_state["prompts"] = st.session_state["prompts"] + [{"role": "assistant", "content": ai_response}]
        
        # ëŒ€í™” ê¸°ë¡ ì—…ë°ì´íŠ¸
        current_time = datetime.now().strftime("%H:%M")
        st.session_state["conversation"] = st.session_state["conversation"] + [("bot", current_time, ai_response)]

        # ëŒ€í™” ê¸°ë¡ì„ ì±„íŒ… í˜•ì‹ìœ¼ë¡œ í‘œì‹œ
        for sender, time, message in st.session_state["conversation"]:
            if sender == "user":
                st.write(f'<div style="display:flex;align-items:center;"><div style="background-color:#007AFF;color:white;border-radius:12px;padding:8px 12px;margin-right:8px;">{message}</div><div style="font-size:0.8rem;color:gray;">{time}</div></div>', unsafe_allow_html=True)
                st.write("")
            else:
                st.write(f'<div style="display:flex;align-items:center;justify-content:flex-end;"><div style="background-color:lightgray;border-radius:12px;padding:8px 12px;margin-left:8px;">{message}</div><div style="font-size:0.8rem;color:gray;">{time}</div></div>', unsafe_allow_html=True)
                st.write("")
        
        # TTSë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜ ë° ì¬ìƒ
        convert_text_to_speech(ai_response, openai_client)
